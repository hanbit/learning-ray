{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "780507ca",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with Ray RLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2856c7b0",
   "metadata": {},
   "source": [
    "\n",
    "You can run this notebook directly in\n",
    "[Colab](https://colab.research.google.com/github/hanbit/learning_ray/blob/main/notebooks/ch_04_rllib.ipynb).\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/hanbit/learning_ray/blob/main/notebooks/ch_04_rllib.ipynb\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea5e1f7",
   "metadata": {},
   "source": [
    "For this chapter you need to install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d6d409",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"ray[rllib]==2.2.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b17e3aa",
   "metadata": {},
   "source": [
    "\n",
    "To import utility files for this chapter, on Colab you will also have to clone\n",
    "the repo and copy the code files to the base path of the runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8206edba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/hanbit/learning_ray\n",
    "%cp -r learning_ray/notebooks/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "class Env:\n",
    "\n",
    "    action_space: gym.spaces.Space\n",
    "    observation_space: gym.spaces.Space\n",
    "\n",
    "    def step(self, action):\n",
    "        ...\n",
    "\n",
    "    def reset(self):\n",
    "        ...\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72525bf",
   "metadata": {},
   "source": [
    "In `maze.py` we set `num_rollout_workers=0` for this notebook, so that the code works in Colab. In the book itself we use 2 rollout workers to show that experience collection can be distributed by RLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a6734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rllib train file maze.py --stop '{\"timesteps_total\": 10000}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dfc658",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "Try:\n",
    "rllib evaluate ~/ray_results/maze_env/<checkpoint>\\\n",
    " --algo DQN\\\n",
    " --env maze_gym_env.Environment\\\n",
    " --steps 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ee003",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from ray.tune.logger import pretty_print\n",
    "from maze_gym_env import GymEnvironment\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "\n",
    "config = (DQNConfig().environment(GymEnvironment)\n",
    "          .rollouts(num_rollout_workers=2, create_env_on_local_worker=True))\n",
    "\n",
    "pretty_print(config.to_dict())\n",
    "\n",
    "algo = config.build()\n",
    "\n",
    "for i in range(10):\n",
    "    result = algo.train()\n",
    "\n",
    "print(pretty_print(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283bf0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "\n",
    "\n",
    "checkpoint = algo.save()\n",
    "print(checkpoint)\n",
    "\n",
    "evaluation = algo.evaluate()\n",
    "print(pretty_print(evaluation))\n",
    "\n",
    "algo.stop()\n",
    "restored_algo = Algorithm.from_checkpoint(checkpoint)\n",
    "\n",
    "algo = restored_algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e3c637",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "env = GymEnvironment()\n",
    "done = False\n",
    "total_reward = 0\n",
    "observations = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = algo.compute_single_action(observations)\n",
    "    observations, reward, done, info = env.step(action)\n",
    "    total_reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6230b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = algo.compute_actions(\n",
    "    {\"obs_1\": observations, \"obs_2\": observations}\n",
    ")\n",
    "print(action)\n",
    "# {'obs_1': 0, 'obs_2': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d2c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = algo.get_policy()\n",
    "print(policy.get_weights())\n",
    "\n",
    "model = policy.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83366455",
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = algo.workers\n",
    "workers.foreach_worker(\n",
    "    lambda remote_trainer: remote_trainer.get_policy().get_weights()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371e167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a539340",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "\n",
    "\n",
    "env = GymEnvironment()\n",
    "obs_space = env.observation_space\n",
    "preprocessor = get_preprocessor(obs_space)(obs_space)\n",
    "\n",
    "observations = env.reset()\n",
    "transformed = preprocessor.transform(observations).reshape(1, -1)\n",
    "\n",
    "model_output, _ = model({\"obs\": transformed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f47a55f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "q_values = model.get_q_value_distributions(model_output)\n",
    "print(q_values)\n",
    "\n",
    "action_distribution = policy.dist_class(model_output, model)\n",
    "sample = action_distribution.sample()\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dc4ca5",
   "metadata": {},
   "source": [
    "\n",
    "![RLlib Environments](https://raw.githubusercontent.com/hanbit/learning_ray/main/notebooks/images/chapter_04/rllib_envs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9081e6a5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from gym.spaces import Discrete\n",
    "import os\n",
    "\n",
    "\n",
    "class MultiAgentMaze(MultiAgentEnv):\n",
    "\n",
    "    def __init__(self,  *args, **kwargs):\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Discrete(5*5)\n",
    "        self.agents = {1: (4, 0), 2: (0, 4)}\n",
    "        self.goal = (4, 4)\n",
    "        self.info = {1: {'obs': self.agents[1]}, 2: {'obs': self.agents[2]}}\n",
    "\n",
    "    def reset(self):\n",
    "        self.agents = {1: (4, 0), 2: (0, 4)}\n",
    "\n",
    "        return {1: self.get_observation(1), 2: self.get_observation(2)}\n",
    "\n",
    "    def get_observation(self, agent_id):\n",
    "        seeker = self.agents[agent_id]\n",
    "        return 5 * seeker[0] + seeker[1]\n",
    "\n",
    "    def get_reward(self, agent_id):\n",
    "        return 1 if self.agents[agent_id] == self.goal else 0\n",
    "\n",
    "    def is_done(self, agent_id):\n",
    "        return self.agents[agent_id] == self.goal\n",
    "\n",
    "    def step(self, action):\n",
    "        agent_ids = action.keys()\n",
    "\n",
    "        for agent_id in agent_ids:\n",
    "            seeker = self.agents[agent_id]\n",
    "            if action[agent_id] == 0:  # move down\n",
    "                seeker = (min(seeker[0] + 1, 4), seeker[1])\n",
    "            elif action[agent_id] == 1:  # move left\n",
    "                seeker = (seeker[0], max(seeker[1] - 1, 0))\n",
    "            elif action[agent_id] == 2:  # move up\n",
    "                seeker = (max(seeker[0] - 1, 0), seeker[1])\n",
    "            elif action[agent_id] == 3:  # move right\n",
    "                seeker = (seeker[0], min(seeker[1] + 1, 4))\n",
    "            else:\n",
    "                raise ValueError(\"Invalid action\")\n",
    "            self.agents[agent_id] = seeker\n",
    "\n",
    "        observations = {i: self.get_observation(i) for i in agent_ids}\n",
    "        rewards = {i: self.get_reward(i) for i in agent_ids}\n",
    "        done = {i: self.is_done(i) for i in agent_ids}\n",
    "\n",
    "        done[\"__all__\"] = all(done.values())\n",
    "\n",
    "        return observations, rewards, done, self.info\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        \"\"\"We override this method here so clear the output in Jupyter notebooks.\n",
    "        The previous implementation works well in the terminal, but does not clear\n",
    "        the screen in interactive environments.\n",
    "        \"\"\"\n",
    "        os.system('cls' if os.name == 'nt' else 'clear')\n",
    "        try:\n",
    "            from IPython.display import clear_output\n",
    "            clear_output(wait=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "        grid = [['| ' for _ in range(5)] + [\"|\\n\"] for _ in range(5)]\n",
    "        grid[self.goal[0]][self.goal[1]] = '|G'\n",
    "        grid[self.agents[1][0]][self.agents[1][1]] = '|1'\n",
    "        grid[self.agents[2][0]][self.agents[2][1]] = '|2'\n",
    "        grid[self.agents[2][0]][self.agents[2][1]] = '|2'\n",
    "        print(''.join([''.join(grid_row) for grid_row in grid]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3057fa",
   "metadata": {},
   "source": [
    "![RLlib Mapping Envs](https://raw.githubusercontent.com/hanbit/learning_ray/main/notebooks/images/chapter_04/mapping_envs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b74fd5b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "env = MultiAgentMaze()\n",
    "\n",
    "while True:\n",
    "    obs, rew, done, info = env.step(\n",
    "        {1: env.action_space.sample(), 2: env.action_space.sample()}\n",
    "    )\n",
    "    time.sleep(0.1)\n",
    "    env.render()\n",
    "    if any(done.values()):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417642b6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "\n",
    "simple_trainer = DQNConfig().environment(env=MultiAgentMaze).build()\n",
    "simple_trainer.train()\n",
    "\n",
    "algo = DQNConfig()\\\n",
    "    .environment(env=MultiAgentMaze)\\\n",
    "    .multi_agent(\n",
    "        policies={\n",
    "            \"policy_1\": (\n",
    "                None, env.observation_space, env.action_space, {\"gamma\": 0.80}\n",
    "            ),\n",
    "            \"policy_2\": (\n",
    "                None, env.observation_space, env.action_space, {\"gamma\": 0.95}\n",
    "            ),\n",
    "        },\n",
    "        policy_mapping_fn = lambda agent_id: f\"policy_{agent_id}\",\n",
    "    ).build()\n",
    "\n",
    "print(algo.train())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b65341b",
   "metadata": {},
   "source": [
    "![RLlib External Envs](https://raw.githubusercontent.com/hanbit/learning_ray/main/notebooks/images/chapter_04/rllib_external.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897b4d78",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "class AdvancedEnv(GymEnvironment):\n",
    "\n",
    "    def __init__(self, seeker=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.maze_len = 11\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Discrete(self.maze_len * self.maze_len)\n",
    "\n",
    "        if seeker:\n",
    "            assert 0 <= seeker[0] < self.maze_len and \\\n",
    "                   0 <= seeker[1] < self.maze_len\n",
    "            self.seeker = seeker\n",
    "        else:\n",
    "            self.reset()\n",
    "\n",
    "        self.goal = (self.maze_len-1, self.maze_len-1)\n",
    "        self.info = {'seeker': self.seeker, 'goal': self.goal}\n",
    "\n",
    "        self.punish_states = [\n",
    "            (i, j) for i in range(self.maze_len) for j in range(self.maze_len)\n",
    "            if i % 2 == 1 and j % 2 == 0\n",
    "        ]\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset seeker position randomly, return observations.\"\"\"\n",
    "        self.seeker = (\n",
    "            random.randint(0, self.maze_len - 1),\n",
    "            random.randint(0, self.maze_len - 1)\n",
    "        )\n",
    "        return self.get_observation()\n",
    "\n",
    "    def get_observation(self):\n",
    "        \"\"\"Encode the seeker position as integer\"\"\"\n",
    "        return self.maze_len * self.seeker[0] + self.seeker[1]\n",
    "\n",
    "    def get_reward(self):\n",
    "        \"\"\"Reward finding the goal and punish forbidden states\"\"\"\n",
    "        reward = -1 if self.seeker in self.punish_states else 0\n",
    "        reward += 5 if self.seeker == self.goal else 0\n",
    "        return reward\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        \"\"\"We override this method here so clear the output in Jupyter notebooks.\n",
    "        The previous implementation works well in the terminal, but does not clear\n",
    "        the screen in interactive environments.\n",
    "        \"\"\"\n",
    "        os.system('cls' if os.name == 'nt' else 'clear')\n",
    "        try:\n",
    "            from IPython.display import clear_output\n",
    "            clear_output(wait=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "        grid = [['| ' for _ in range(self.maze_len)] +\n",
    "                [\"|\\n\"] for _ in range(self.maze_len)]\n",
    "        for punish in self.punish_states:\n",
    "            grid[punish[0]][punish[1]] = '|X'\n",
    "        grid[self.goal[0]][self.goal[1]] = '|G'\n",
    "        grid[self.seeker[0]][self.seeker[1]] = '|S'\n",
    "        print(''.join([''.join(grid_row) for grid_row in grid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec1d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.env.apis.task_settable_env import TaskSettableEnv\n",
    "\n",
    "\n",
    "class CurriculumEnv(AdvancedEnv, TaskSettableEnv):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        AdvancedEnv.__init__(self)\n",
    "\n",
    "    def difficulty(self):\n",
    "        return abs(self.seeker[0] - self.goal[0]) + \\\n",
    "               abs(self.seeker[1] - self.goal[1])\n",
    "\n",
    "    def get_task(self):\n",
    "        return self.difficulty()\n",
    "\n",
    "    def set_task(self, task_difficulty):\n",
    "        while not self.difficulty() <= task_difficulty:\n",
    "            self.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dcd250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def curriculum_fn(train_results, task_settable_env, env_ctx):\n",
    "    time_steps = train_results.get(\"timesteps_total\")\n",
    "    difficulty = time_steps // 1000\n",
    "    print(f\"Current difficulty: {difficulty}\")\n",
    "    return difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db20214",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "import tempfile\n",
    "\n",
    "\n",
    "temp = tempfile.mkdtemp()\n",
    "\n",
    "trainer = (\n",
    "    DQNConfig()\n",
    "    .environment(env=CurriculumEnv, env_task_fn=curriculum_fn)\n",
    "    .offline_data(output=temp)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(15):\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e878b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "imitation_algo = (\n",
    "    DQNConfig()\n",
    "    .environment(env=AdvancedEnv)\n",
    "    .evaluation(off_policy_estimation_methods={})\n",
    "    .offline_data(input_=temp)\n",
    "    .exploration(explore=False)\n",
    "    .build())\n",
    "\n",
    "for i in range(10):\n",
    "    imitation_algo.train()\n",
    "\n",
    "imitation_algo.evaluate()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
