{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/TeeDDub/269636150e58c58748615c443d898d63/ch_07_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89466d94",
      "metadata": {
        "id": "89466d94"
      },
      "source": [
        "# Distributed Training with Ray Train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c23b3af",
      "metadata": {
        "id": "2c23b3af"
      },
      "source": [
        "\n",
        "You can run this notebook directly in\n",
        "[Colab](https://colab.research.google.com/github/hanbit/learning-ray/blob/main/notebooks/ch_07_train.ipynb).\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/hanbit/learning-ray/blob/main/notebooks/ch_07_train.ipynb\">\n",
        "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94e05d4f",
      "metadata": {
        "id": "94e05d4f"
      },
      "source": [
        "For this chapter you will need to install the following dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1765ed6f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1765ed6f",
        "outputId": "9b424c13-7193-4b1e-f5b0-a34145cb5f12"
      },
      "outputs": [],
      "source": [
        "! pip install \"ray[data,train]==2.2.0\" \"dask==2022.8.1\" \"torch==1.12.1\"\n",
        "! pip install \"xgboost==1.6.2\" \"xgboost-ray==0.1.10\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28ef63a5",
      "metadata": {
        "id": "28ef63a5"
      },
      "source": [
        "\n",
        "To import utility files for this chapter, on Colab you will also have to clone\n",
        "the repo and copy the code files to the base path of the runtime:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a179146",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a179146",
        "outputId": "eede4d69-ea26-400c-af0d-f46b2e875551"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/hanbit/learning-ray\n",
        "%cp -r learning-ray/notebooks/* ."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Eqf2icAiHA8v",
      "metadata": {
        "collapsed": false,
        "id": "Eqf2icAiHA8v"
      },
      "source": [
        "![Data Model Parallel](https://raw.githubusercontent.com/hanbit/learning-ray/main/notebooks/images/chapter_07/data_model_parallel.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NQYgtbnMHA8w",
      "metadata": {
        "collapsed": false,
        "id": "NQYgtbnMHA8w"
      },
      "source": [
        "![Torch Trainer](https://raw.githubusercontent.com/hanbit/learning-ray/main/notebooks/images/chapter_07/torch_trainer.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lf0o9MaAHA8w",
      "metadata": {
        "collapsed": false,
        "id": "lf0o9MaAHA8w"
      },
      "source": [
        "![Train Architecture](https://raw.githubusercontent.com/hanbit/learning-ray/main/notebooks/images/chapter_07/train_architecture.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hnCRsRj7HA8w",
      "metadata": {
        "collapsed": false,
        "id": "hnCRsRj7HA8w"
      },
      "source": [
        "![Train Overview](https://raw.githubusercontent.com/hanbit/learning-ray/main/notebooks/images/chapter_07/train_overview.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fd300NDkHA8w",
      "metadata": {
        "collapsed": false,
        "id": "Fd300NDkHA8w"
      },
      "source": [
        "![Train Tune Execution](https://raw.githubusercontent.com/hanbit/learning-ray/main/notebooks/images/chapter_07/train_tune_execution.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "409cb5d3",
      "metadata": {
        "id": "409cb5d3"
      },
      "outputs": [],
      "source": [
        "import ray\n",
        "from ray.util.dask import enable_dask_on_ray\n",
        "\n",
        "import dask.dataframe as dd\n",
        "\n",
        "LABEL_COLUMN = \"is_big_tip\"\n",
        "FEATURE_COLUMNS = [\"passenger_count\", \"trip_distance\", \"fare_amount\",\n",
        "                   \"trip_duration\", \"hour\", \"day_of_week\"]\n",
        "\n",
        "enable_dask_on_ray()\n",
        "\n",
        "\n",
        "def load_dataset(path: str, *, include_label=True):\n",
        "    columns = [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"tip_amount\",\n",
        "               \"passenger_count\", \"trip_distance\", \"fare_amount\"]\n",
        "    df = dd.read_parquet(path, columns=columns)\n",
        "\n",
        "    df = df.dropna()\n",
        "    df = df[(df[\"passenger_count\"] <= 4) &\n",
        "            (df[\"trip_distance\"] < 100) &\n",
        "            (df[\"fare_amount\"] < 1000)]\n",
        "\n",
        "    df[\"tpep_pickup_datetime\"] = dd.to_datetime(df[\"tpep_pickup_datetime\"])\n",
        "    df[\"tpep_dropoff_datetime\"] = dd.to_datetime(df[\"tpep_dropoff_datetime\"])\n",
        "\n",
        "    df[\"trip_duration\"] = (df[\"tpep_dropoff_datetime\"] -\n",
        "                           df[\"tpep_pickup_datetime\"]).dt.seconds\n",
        "    df = df[df[\"trip_duration\"] < 4 * 60 * 60] # 4 hours.\n",
        "    df[\"hour\"] = df[\"tpep_pickup_datetime\"].dt.hour\n",
        "    df[\"day_of_week\"] = df[\"tpep_pickup_datetime\"].dt.weekday\n",
        "\n",
        "    if include_label:\n",
        "        df[LABEL_COLUMN] = df[\"tip_amount\"] > 0.2 * df[\"fare_amount\"]\n",
        "\n",
        "    df = df.drop(\n",
        "        columns=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"tip_amount\"]\n",
        "    )\n",
        "\n",
        "    return ray.data.from_dask(df).repartition(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cd15858",
      "metadata": {
        "id": "7cd15858"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from ray.air import session\n",
        "from ray.air.config import ScalingConfig\n",
        "import ray.train as train\n",
        "from ray.train.torch import TorchCheckpoint, TorchTrainer\n",
        "from fare_predictor import FarePredictor\n",
        "\n",
        "def train_loop_per_worker(config: dict):\n",
        "    batch_size = config.get(\"batch_size\", 32)\n",
        "    lr = config.get(\"lr\", 1e-2)\n",
        "    num_epochs = config.get(\"num_epochs\", 3)\n",
        "\n",
        "    dataset_shard = session.get_dataset_shard(\"train\")\n",
        "\n",
        "    model = FarePredictor()\n",
        "    dist_model = train.torch.prepare_model(model)\n",
        "\n",
        "    loss_function = nn.SmoothL1Loss()\n",
        "    optimizer = torch.optim.Adam(dist_model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        loss = 0\n",
        "        num_batches = 0\n",
        "        for batch in dataset_shard.iter_torch_batches(\n",
        "                batch_size=batch_size, dtypes=torch.float\n",
        "        ):\n",
        "            labels = torch.unsqueeze(batch[LABEL_COLUMN], dim=1)\n",
        "            inputs = torch.cat(\n",
        "                [torch.unsqueeze(batch[f], dim=1) for f in FEATURE_COLUMNS], dim=1\n",
        "            )\n",
        "            output = dist_model(inputs)\n",
        "            batch_loss = loss_function(output, labels)\n",
        "            optimizer.zero_grad()\n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            num_batches += 1\n",
        "            loss += batch_loss.item()\n",
        "\n",
        "        session.report(\n",
        "            {\"epoch\": epoch, \"loss\": loss},\n",
        "            checkpoint=TorchCheckpoint.from_model(dist_model)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5209607a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "5209607a",
        "lines_to_next_cell": 2,
        "outputId": "18c2c711-4e8d-4676-8c54-9c252e2a0d44"
      },
      "outputs": [],
      "source": [
        "# NOTE: In the book we use num_workers=2, but reduce this here, so that it runs on Colab.\n",
        "# In any case, this training loop will take considerable time to run.\n",
        "trainer = TorchTrainer(\n",
        "    train_loop_per_worker=train_loop_per_worker,\n",
        "    train_loop_config={\n",
        "        \"lr\": 1e-2, \"num_epochs\": 3, \"batch_size\": 64\n",
        "    },\n",
        "    scaling_config=ScalingConfig(num_workers=1, resources_per_worker={\"CPU\": 1, \"GPU\": 0}),\n",
        "    datasets={\n",
        "        \"train\": load_dataset(\"nyc_tlc_data/yellow_tripdata_2020-01.parquet\")\n",
        "    },\n",
        ")\n",
        "\n",
        "result = trainer.fit()\n",
        "trained_model = result.checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88deaa0b",
      "metadata": {
        "id": "88deaa0b",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "from ray.train.torch import TorchPredictor\n",
        "from ray.train.batch_predictor import BatchPredictor\n",
        "\n",
        "batch_predictor = BatchPredictor(trained_model, TorchPredictor)\n",
        "ds = load_dataset(\n",
        "    \"nyc_tlc_data/yellow_tripdata_2021-01.parquet\", include_label=False)\n",
        "\n",
        "batch_predictor.predict_pipelined(ds, blocks_per_window=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4087d98b",
      "metadata": {
        "id": "4087d98b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from ray.data import from_torch\n",
        "\n",
        "num_samples = 20\n",
        "input_size = 10\n",
        "layer_size = 15\n",
        "output_size = 5\n",
        "num_epochs = 3\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_size, layer_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(layer_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def train_data():\n",
        "    return torch.randn(num_samples, input_size)\n",
        "\n",
        "\n",
        "input_data = train_data()\n",
        "label_data = torch.randn(num_samples, output_size)\n",
        "train_dataset = from_torch(input_data)\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loss_fn, optimizer):\n",
        "    output = model(input_data)\n",
        "    loss = loss_fn(output, label_data)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "def training_loop():\n",
        "    model = NeuralNetwork()\n",
        "    loss_fn = nn.MSELoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "    for epoch in range(num_epochs):\n",
        "        train_one_epoch(model, loss_fn, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cf11c52",
      "metadata": {
        "id": "0cf11c52"
      },
      "outputs": [],
      "source": [
        "from ray.train.torch import prepare_model\n",
        "\n",
        "\n",
        "def distributed_training_loop():\n",
        "    model = NeuralNetwork()\n",
        "    model = prepare_model(model)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "    for epoch in range(num_epochs):\n",
        "        train_one_epoch(model, loss_fn, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afbc5a35",
      "metadata": {
        "id": "afbc5a35",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "from ray.air.config import ScalingConfig\n",
        "from ray.train.torch import TorchTrainer\n",
        "\n",
        "\n",
        "trainer = TorchTrainer(\n",
        "    train_loop_per_worker=distributed_training_loop,\n",
        "    scaling_config=ScalingConfig(\n",
        "        num_workers=2,\n",
        "        use_gpu=False\n",
        "    ),\n",
        "    datasets={\"train\": train_dataset}\n",
        ")\n",
        "\n",
        "result = trainer.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c049af7",
      "metadata": {
        "id": "3c049af7",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import ray\n",
        "\n",
        "from ray.air.config import ScalingConfig\n",
        "from ray import tune\n",
        "from ray.data.preprocessors import StandardScaler, MinMaxScaler\n",
        "\n",
        "\n",
        "dataset = ray.data.from_items(\n",
        "    [{\"X\": x, \"Y\": 1} for x in range(0, 100)] +\n",
        "    [{\"X\": x, \"Y\": 0} for x in range(100, 200)]\n",
        ")\n",
        "prep_v1 = StandardScaler(columns=[\"X\"])\n",
        "prep_v2 = MinMaxScaler(columns=[\"X\"])\n",
        "\n",
        "param_space = {\n",
        "    \"scaling_config\": ScalingConfig(\n",
        "        num_workers=tune.grid_search([2, 4]),\n",
        "        resources_per_worker={\n",
        "            \"CPU\": 2,\n",
        "            \"GPU\": 0,\n",
        "        },\n",
        "    ),\n",
        "    \"preprocessor\": tune.grid_search([prep_v1, prep_v2]),\n",
        "    \"params\": {\n",
        "        \"objective\": \"binary:logistic\",\n",
        "        \"tree_method\": \"hist\",\n",
        "        \"eval_metric\": [\"logloss\", \"error\"],\n",
        "        \"eta\": tune.loguniform(1e-4, 1e-1),\n",
        "        \"subsample\": tune.uniform(0.5, 1.0),\n",
        "        \"max_depth\": tune.randint(1, 9),\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9e305d2",
      "metadata": {
        "id": "c9e305d2"
      },
      "outputs": [],
      "source": [
        "from ray.train.xgboost import XGBoostTrainer\n",
        "from ray.air.config import RunConfig\n",
        "from ray.tune import Tuner\n",
        "\n",
        "\n",
        "trainer = XGBoostTrainer(\n",
        "    params={},\n",
        "    run_config=RunConfig(verbose=2),\n",
        "    preprocessor=None,\n",
        "    scaling_config=None,\n",
        "    label_column=\"Y\",\n",
        "    datasets={\"train\": dataset}\n",
        ")\n",
        "\n",
        "tuner = Tuner(\n",
        "    trainer,\n",
        "    param_space=param_space,\n",
        ")\n",
        "\n",
        "results = tuner.fit()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
